"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[1477],{10:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"kqir-query-engine","metadata":{"permalink":"/blog/kqir-query-engine","editUrl":"https://github.com/apache/kvrocks-website/tree/main/blog/2024-06-02-kqir-query-engine/index.md","source":"@site/blog/2024-06-02-kqir-query-engine/index.md","title":"KQIR: a query engine for Apache Kvrocks that supports both SQL and RediSearch queries","description":"Intro","date":"2024-06-02T00:00:00.000Z","formattedDate":"June 2, 2024","tags":[],"readingTime":9.105,"hasTruncateMarker":true,"authors":[{"name":"PragmaTwice","title":"Apache Kvrocks PMC Member","url":"https://github.com/pragmatwice","imageURL":"https://github.com/pragmatwice.png","key":"twice"}],"frontMatter":{"slug":"kqir-query-engine","title":"KQIR: a query engine for Apache Kvrocks that supports both SQL and RediSearch queries","authors":["twice"]},"nextItem":{"title":"Apache Kvrocks 2023 In Review","permalink":"/blog/kvrocks-2023-in-review"}},"content":"## Intro\\n\\nTL;DR:\\n\\n![ftsearch-demo](./images/ftsearch-demo.png)\\n\\nPretty cool, right? Let\'s dive in!\\n\\n(The full example is provided in [the final section](/blog/kqir-query-engine/#try-it).)\\n\\n\x3c!--truncate--\x3e\\n\\n### Apache Kvrocks\\n\\n[Apache Kvrocks](https://kvrocks.apache.org/) is a [Redis](https://redis.io/)-compatible database built on [RocksDB](https://rocksdb.org/).\\n\\nIt supports [the RESP protocol](https://redis.io/docs/latest/develop/reference/protocol-spec/) (version 2 and 3) and [a wide range of Redis commands](/docs/supported-commands), encompassing core data structures like Strings, Sets, Hashes, Sorted Sets, Stream, GEO, as well as Lua Scripts, Transactions, [Functions](https://redis.io/docs/latest/develop/interact/programmability/functions-intro/) and even [BloomFilter](https://redis.io/docs/latest/develop/data-types/probabilistic/bloom-filter/), [JSON](https://redis.io/docs/latest/develop/data-types/json/) from the Redis Stack.\\n\\nUnlike Redis which stores data in memory, Kvrocks persists data on disk for improved storage capabilities without being constrained by machine memory limit.\\n\\n### The capability to query\\n\\nIn recent years, NoSQL databases have become more popular than traditional databases because they perform better, scale easily, and are more flexible for different industries.\\n\\nHowever, many users are unwilling to abandon the essential features of SQL databases just for performance reasons.\\nThese include ACID transactions, expressive query capabilities inherent in SQL, as well as optimization and abstraction possibilities offered by structured data and relational algebra.\\nConsequently, a new category of databases known as NewSQL has emerged gradually.\\n\\nKvrocks is a NoSQL database.\\nWhile not classified as NewSQL, Kvrocks aims to strike a balance between NoSQL and NewSQL paradigms:\\nIt aims to maintain the high performance of NoSQL while also implementing transaction guarantees and supporting more complex queries.\\n\\n### RediSearch?\\n\\n[RediSearch](https://github.com/RediSearch/RediSearch) is a Redis module that enhances Redis with query, secondary indexing, and full-text search functionalities. \\nWhile [its Redis commands](https://redis.io/docs/latest/operate/oss_and_stack/stack-with-enterprise/search/commands/) begin with `FT.` (i.e. full text), it goes beyond just full-text search.\\n\\nIn fact, it is Redis moving closer to SQL databases:\\nRediSearch enables users to create structured schemas on existing Redis JSON or HASH data for index building.\\nIts schema supports [various field types](https://redis.io/docs/latest/develop/interact/search-and-query/basic-constructs/field-and-type-options/) such as numeric, tag, geo, text, and vector - the latter two are utilized for full-text and vector searches.\\nInstead of SQL support, RediSearch provides [a unique query syntax](https://redis.io/docs/latest/develop/interact/search-and-query/advanced-concepts/query_syntax/) known as the RediSearch query language.\\n\\nRediSearch finds applications in various fields.\\nOne recent application involves utilizing its vector search feature to develop retrieval-augmented generation (RAG). For instance, [LangChain](https://www.langchain.com/) utilizes Redis as one of its vector database.\\nIf Kvrocks can be compatible with RediSearch, it could benefit from these ecosystem from RediSearch.\\n\\n### SQL?\\n\\nRediSearch uses a unique syntax for queries, but there are some issues to consider:\\n\\nFirstly, RediSearch\'s schema (a.k.a. index, created with `FT.CREATE`) can be regarded as a table in an SQL database. Its query syntax also aligns semantically with SQL queries.\\nGiven this similarity, supporting SQL should not increase significant challenges; why not include SQL support as well?\\n\\nSecondly, SQL enjoys broader usage and is familiar to more individuals. It is simpler to learn at the syntax level. While developers may need time to understand RediSearch query syntax, adapting to a new SQL database often requires less effort. Furthermore, SQL offers robust support for various query features, enhanced expressive capabilities (like JOINs, subqueries, aggregations).\\n\\nFinally, RediSearch query syntax suffers from some historical designs. For example, the operator precedence of AND and OR (represented by space and `|` operator in RediSearch queries) [varies across different dialect versions](https://redis.io/docs/latest/develop/interact/search-and-query/advanced-concepts/query_syntax/#basic-syntax) (dialect 1 vs. dialect 2). This tribal knowledge might lead users to prefer established query languages.\\n\\nTo sum up, we believe that supporting SQL as a querying language would be a good decision.\\n\\n## How we support both?\\n\\n![KQIR](./images/KQIR.png)\\n\\nTo introduce SQL capability to Kvrocks, we need to design a robust architecture with scalability, maintainability, and strong query planning and optimization features.\\n\\nWe plan to accomplish this through [KQIR](https://github.com/apache/kvrocks/tree/unstable/src/search). In the context of Kvrocks, KQIR stands for both:\\n1. The complete query engine, covering frontend language parsing, query optimization and execution, etc.\\n2. An intermediate language (IR) that traverses the entire query engine.\\n\\n### KQIR: a multiple-level IR\\n\\nTo support both SQL and RediSearch queries, an intermediate language (IR) is necessary to handle them consistently in subsequent processes without concern for the user\'s input language.\\n\\nWe have developed parsers for a subset of MySQL syntax and RediSearch queries, converting the resulting syntax tree into KQIR.\\n\\nAnd KQIR is a multi-level IR that can represent query structures at various levels during optimization.\\nThe initial transformation from the syntax tree results in Syntactical IR, a high-level representation of certain syntactic expressions.\\nAs it undergoes processing by an IR optimizer, KQIR evolves into Planning IR, a low-level representation used to express query execution plans within the query engine.\\n\\nAdditionally, we will conduct semantic checks on the IR before optimization to ensure that the query is semantically correct.\\nThis includes verifying that it does not include any undefined schemas or fields and uses the appropriate field types.\\n\\n### IR Optimizer\\n\\nThe KQIR optimizer consists of multiple passes, [a concept borrowed from LLVM](https://llvm.org/docs/Passes.html).\\nEach pass takes IR as input, conducts analysis and modifications, and generates a new IR.\\n\\nCurrently, the optimizer\'s passes are categorized into three main groups:\\n- expression passes for optimizing logical expressions like `AND`, `OR`, `NOT` operators;\\n- numeric passes for optimizing numerical comparisons with an interval analysis (i.e. analyze the mathematical properties of numerical comparisons in terms of intervals) to enhance query optimization by eliminating unnecessary comparisons or improving comparison expressions;\\n- planning passes for converting syntactical IR to planning IR and enhancing query plans through a cost model that selects optimal indexes and removes unnecessary sortings.\\n\\nPass execution order is controlled by the pass manager.\\nA pass may run multiple times at different stages to simplify individual passes by combining them.\\n\\n### Plan Executor\\n\\nThe KQIR plan executor is built on the Volcano model.\\n\\nOnce the IR optimizer finishes all optimizations, the resulting IR becomes a planning IR. This will then be passed to the plan executor to create execution logic based on certain context corresponding to the plan operator.\\n\\nSubsequently, Kvrocks retrieves query results through iterative execution.\\n\\n### On-disk indexing\\n\\nUnlike Redis, which stores index data in memory, Kvrocks requires the construction of indexes on the disk.\\nThis means that for any field type (e.g. tag, numeric), we need an encoding to reduce such index to RocksDB\'s key-values.\\n\\nFurthermore, we incrementally create indexes before and after JSON or HASH commands getting executed to guarantee that query results are in real-time.\\n\\n## Current status\\n\\nThe KQIR functionality is currently available on the `unstable` branch, supporting commands like `FT.CREATE`, `FT.SEARCH`, and `FT.SEARCHSQL` (an extension for running SQL queries) to encourage user to test.\\n\\nHowever, as KQIR is still in early development, compatibility cannot be guaranteed and many features remain incomplete.\\nThus the upcoming release (version 2.9.0) will not include any KQIR component.\\n\\n### Supported field types\\n\\nCurrently, we only support two field types: tag and numeric.\\n\\nTag fields label each data record with multiple tags for filtering in queries.\\nAnd numeric fields hold numerical data within double-precision floating-point ranges. They allow sorting and filtering by specific numerical ranges.\\n\\nIn the future, we plan to expand support to include vector search and full-text search capabilities alongside other field types.\\n\\n### Transaction guarantees\\n\\nCurrently, the transaction guarantee of KQIR is weak, which may lead to unexpected issues during use.\\n\\n[Another project in the Kvrocks community](https://github.com/apache/kvrocks/issues/2331) aims to enhance Kvrocks\' transaction support by establishing a structured framework.\\nWe will leverage these efforts to uphold the ACID properties of KQIR and release an official version incorporating KQIR after that.\\n\\n### Limitation on IR optimizer\\n\\nCurrently, KQIR does not use the cost model when optimizing record sorting.\\nInstead, it relies on specialized logic. This could be an area for improvement soon.\\n\\nFurthermore, KQIR does not currently utilize optimizations based on runtime statistics.\\nOur future focus will be on integrating runtime statistics into the cost model for more precise index selection.\\n\\n### Relationship with other features\\n\\nKQIR integrates well with the [namespace](https://kvrocks.apache.org/docs/namespace) feature.\\nAny index created is restricted to the current namespace and cannot be accessed in other namespaces, aligning with how other data is accessed within the namespace.\\n\\nCurrently, KQIR cannot be enabled in the [cluster mode](https://kvrocks.apache.org/docs/cluster).\\nCluster mode support may not be implemented in the short term, but we encourage anyone to submit discussions, design proposals, or suggestions.\\n\\n### Compliance\\n\\nWhile KQIR is designed to be compatible with RediSearch at the interface level, it does not include any code from RediSearch.\\nAs previously mentioned, KQIR features a completely new framework, and its query architecture (including parsing, optimization, execution) is independent of RediSearch.\\n\\nThis distinction is important due to the proprietary license under which RediSearch is released.\\n\\n### High experimental!\\n\\nThe current implementation of KQIR is in its early experimental stage.\\nWe advise users to consider carefully when using KQIR functionalities in a production environment, as we do not guarantee compatibility, and there may be unexpected errors.\\n\\n## Future outlook\\n\\nKQIR is currently in development, and all mentioned aspects will continue to evolve.\\nIf you\'re interested, please stay updated on the progress.\\n\\nDevelopers keen on KQIR are encouraged to get involved in the development process and join the Apache Kvrocks community.\\n\\nNote that our community consists entirely of volunteers.\\nAs an ASF community, we strive to offer an open, inclusive, and vendor-neutral environment.\\n\\n### Vector search\\n\\nThe design and implementation of vector search support are currently underway, which is very exciting.\\n\\nIn the Kvrocks community, some members have raised discussions and [proposed an encoding design](https://github.com/apache/kvrocks/discussions/2316) for implementing vector search on KQIR.\\n\\nAs per the plan, we will initially implement an on-disk HNSW index and introduce the vector field type.\\n\\n### Full-text search\\n\\nThere is currently no design proposal for full-text search.\\n\\nHowever, community members are exploring the potential of incorporating full-text indexing in KQIR via [CLucene](https://clucene.sourceforge.net/) or [PISA](https://github.com/pisa-engine/pisa).\\n\\nWe encourage anyone interested to share their ideas or suggestions and get involved in the development and implementation.\\n\\n### More SQL features\\n\\nIn the future, we aim to progressively broaden our support for SQL features, potentially encompassing subqueries (including common table expressions), JOIN operations, aggregation functions, and other functionalities.\\n\\nOur primary focus will remain on transaction processing rather than analytical tasks.\\n\\n## Try it!\\n\\nFirst, we can easily set up a Kvrocks instance via Docker images.\\nYou also have the choice to manually build executable from the source code in the \'unstable\' branch.\\n\\n```\\ndocker run -it -p 6666:6666 apache/kvrocks:nightly --log-dir stdout\\n```\\n\\nThen, we can connect to kvrocks locally using `redis-cli`,\\nand create an index named `testidx` consisting a tag field `a` and numeric field `b` with the following command:\\n```\\nFT.CREATE testidx ON JSON PREFIX 1 \'test:\' SCHEMA a TAG b NUMERIC\\n```\\n\\nNext, we can add some new data using Redis JSON commands:\\n(Note that it is also possible to add data before running `FT.CREATE`.)\\n```\\nJSON.SET test:k1 $ \'{\\"a\\": \\"x,y\\", \\"b\\": 11}\'\\nJSON.SET test:k2 $ \'{\\"a\\": \\"y,z\\", \\"b\\": 22}\'\\nJSON.SET test:k3 $ \'{\\"a\\": \\"x,z\\", \\"b\\": 33}\'\\n```\\n\\nFinally, we can execute some SQL queries to get the desired results:\\n```\\nFT.SEARCHSQL \'select * from testidx where a hastag \\"z\\" and b < 30\'\\n```\\n\\nOr an equivalent RediSearch query:\\n```\\nFT.SEARCH testidx \'@a:{z} @b:[-inf (30]\'\\n``` \\n\\nEnjoy it!"},{"id":"kvrocks-2023-in-review","metadata":{"permalink":"/blog/kvrocks-2023-in-review","editUrl":"https://github.com/apache/kvrocks-website/tree/main/blog/2024-01-07-kvrocks-2023-in-review/index.md","source":"@site/blog/2024-01-07-kvrocks-2023-in-review/index.md","title":"Apache Kvrocks 2023 In Review","description":"The year 2023 is a small milestone for the community in open source, and it is also a new starting point. Kvrocks successfully graduated from the incubator to become an Apache top-level project in June, which is a great affirmation for the community\'s health and sustainability. At the same time, there are also some exciting progress in the project and community.","date":"2024-01-07T00:00:00.000Z","formattedDate":"January 7, 2024","tags":[],"readingTime":2.835,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"kvrocks-2023-in-review","title":"Apache Kvrocks 2023 In Review"},"prevItem":{"title":"KQIR: a query engine for Apache Kvrocks that supports both SQL and RediSearch queries","permalink":"/blog/kqir-query-engine"},"nextItem":{"title":"Kvrocks graduated as an Apache Top-Level-Project","permalink":"/blog/kvrocks-graduated-as-tlp"}},"content":"The year 2023 is a small milestone for the community in open source, and it is also a new starting point. **Kvrocks successfully graduated from the incubator to become an Apache top-level project in June, which is a great affirmation for the community\'s health and sustainability**. At the same time, there are also some exciting progress in the project and community.\\n\\nBefore we start, we would like to thanks all the contributors and users who have been supporting Apache Kvrocks. We can\'t make it without you!\\n\\n\x3c!--truncate--\x3e\\n\\n### Project Retrospective\\n\\nWe collected a lot of feedback from users and developers in the discussion: [2023 Planning(Want your feedback)](https://github.com/apache/kvrocks/discussions/1226), some of them have been resolved in 2023, and some of them are still in the plan list.\\n\\n**The key progresses in 2023:**\\n\\n- JSON data structure\\n- BloomFilter data structure\\n- Watch/Unwatch command\\n- Lua Functions\\n- Replication with TLS\\n- Allow to enable 64 bit expiration time (precision is in milliseconds instead of seconds), and the length of List/Hash/Set/ZSET/Stream can be also extended from 32 to 64 bits.\\n- Allow to use `LOAD RDB` command to load Redis RDB file\\n- Allow to dynamically adjust the number of Worker threads\\n- Enable Namespace replication\\n- Support RESTORE command\\n\\n**What we are still working on:**\\n\\n- RESP3 protocol, more information can be found in [Tracking issue: Support RESP3 in Kvrocks](https://github.com/apache/kvrocks/issues/1980)\\n- Kvrocks in Kubernetes, [kvrocks-operator](https://github.com/RocksLabs/kvrocks-operator) is ready to test now, but still need more work to make it production-ready\\n- Controller to manage cluster, the API part of [kvrocks-controller](https://github.com/RocksLabs/kvrocks-controller) is finished, but we belive it\'s not enough of users before UI part is ready\\n\\n**What still in the backlog:**\\n\\n- Allow to use the Raft protocol for replication\\n- HyperLogLog\\n- RedisGraph, no plan to support since it\'s deprecated by the Redis community\\n- Semi-sync replication\\n- Client-side caching\\n\\n## Community\\n\\nIn the past year, in addition to operating based on the principle of \\"Community over Code\\" of the Apache Foundation, the community maintenance also adheres to the attitude of openness, transparency, openness, inclusiveness, freedom and equality towards everyone.\\n\\nIn 2023, the community has many things worth sharing:\\n\\n- Successfully graduated from the incubator to become an Apache top-level project in June\\n- Released a total of `6` versions from **2.3.0 - 2.7.0**\\n- Voted for `5` new Committers, they are: xiaobiaozhao / mwish / Aleks Lozovyuk / binbin / yangshixi\\n- The total number of contributors exceeded `100+`, which increased by `41` compared to 2022\\n- Merged PR total number [500+](https://github.com/RocksLabs/kvrocks-operator)\\n- In terms of community communication groups, the number of WeChat groups is `450+`, and the number of Slack is `250+`\\n\\n## New Logo\\n\\nThe number of users in the community has increased by `7` organizations, including Opera / iFlytek / SHOPLAZZA / Netease Hangzhou Research Institute / ZTO Express / Coin Index and AHOY.\\n\\n![New Users](images/new_users.png)\\n\\nThanks to the above users for their use and active feedback, and welcome more users to leave your usage scenarios in GitHub Issue: [Who is using Kvrocks?](https://github.com/apache/kvrocks/issues/414)\\n\\n## 2024 Planning\\n\\nIn the past few years, the annual plan of Kvrocks has also maintained the principle of openness and transparency, and I hope to hear the voices of users and developers. Therefore, the community is open to collect needs and feedback from everyone.\\n\\nPlease don\'t hesitate to leave comments in [2024 Planning(Want your feedback)](https://github.com/apache/kvrocks/discussions/1974) if you have any ideas or suggestions."},{"id":"kvrocks-graduated-as-tlp","metadata":{"permalink":"/blog/kvrocks-graduated-as-tlp","editUrl":"https://github.com/apache/kvrocks-website/tree/main/blog/2023-07-01-kvrocks-graduated-as-tlp/index.md","source":"@site/blog/2023-07-01-kvrocks-graduated-as-tlp/index.md","title":"Kvrocks graduated as an Apache Top-Level-Project","description":"Wilmington, DE \u2013 June 28, 2023 \u2013 The Apache Software Foundation(ASF) announced Kvrocks has graduated from the incubator as a Top-Level-Project. Means that the Kvrocks community has met the Apache Foundation\'s requirements for The Apache Way practices, diversity, and open communication. Graduation marks a new starting point, while much work is still necessary for the community\'s long-term health.","date":"2023-07-01T00:00:00.000Z","formattedDate":"July 1, 2023","tags":[],"readingTime":5.775,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"kvrocks-graduated-as-tlp","title":"Kvrocks graduated as an Apache Top-Level-Project"},"prevItem":{"title":"Apache Kvrocks 2023 In Review","permalink":"/blog/kvrocks-2023-in-review"},"nextItem":{"title":"Getting started with Kvrocks and go-redis","permalink":"/blog/go-redis-kvrocks-opentelemetry"}},"content":"[Wilmington, DE \u2013 June 28, 2023 \u2013 The Apache Software Foundation(ASF) announced Kvrocks has graduated from the incubator as a Top-Level-Project.](https://news.apache.org/foundation/entry/the-apache-software-foundation-announces-new-top-level-project-apache-kvrocks) Means that the Kvrocks community has met the Apache Foundation\'s requirements for The Apache Way practices, diversity, and open communication. Graduation marks a new starting point, while much work is still necessary for the community\'s long-term health.\\n\\n\x3c!--truncate--\x3e\\n\\n### Kvrocks Overview & Advantages\\n\\nKvrocks is a distributed key-value NoSQL database that uses RocksDB as its storage engine and supports the Redis protocol. Compared to Redis, Kvrocks allows users to decrease memory costs and increase capacity.\\n\\n### Kvrocks Feature Highlights\\n\\n- Redis Compatible: Support common Redis data types and commands;\\n- Namespace: Similar to Redis DB but equipped with a token per namespace;\\n- Replication: Async replication using WAL of RocksDB;\\n- High Availability: Support Redis sentinel to failover when master failed; and\\n- Cluster: Centralized management but accessible via any Redis cluster client.\\n\\n### ADDITIONAL RESOURCES\\n\\n- GitHub: [https://github.com/apache/kvrocks](https://github.com/apache/kvrocks)\\n- Download: [https://kvrocks.apache.org/download](https://kvrocks.apache.org/download)\\n- Docs: [https://kvrocks.apache.org/](https://kvrocks.apache.org/)\\n- Contribute: [https://kvrocks.apache.org/community/contributing](https://kvrocks.apache.org/community/contributing)\\n\\nSince being open-sourced in 2019, Kvrocks has been serving as an alternative for Redis in massive data scenarios. Many companies are deploying and using Kvrocks in the production environment, such as Baidu, Circl.lu, Ctrip, Meitu, Opera, U-Next and Xueqiu, among others.\\n\\nTo serve users better, Kvrocks plans to add Kubernetes deployment support, the controller to make the cluster easier to maintain and operate, and add more data structures for fulfilling the user requirements.\\n\\n### Incubation processes\\n\\nKvrocks joined the Apache Incubator at the end of April 2022 and officially became an Apache top-level project in June 2023. There has been a significant increase in community activity\\n\\n1. The number of contributors has increased from **27** to **82**\\n2. Released **4** minor versions during incubation: 2.1.0 / 2.2.0 / 2.3.0 / 2.4.0\\n3. Created **900+** Pull Requests\\n4. Created **300+** Issues\\n5. Nominated **4** Committers: PragmaTwice / Torwig / Ruixiang Tan / Xiaobiao Zhao, and PragmaTwice and Torwig are also PMC members now.\\n\\n![image](images/contributions.png)\\n\\n### Our Users\\n\\nIt\'s worth noting that the majority of Kvrocks community contributors and committers are from the community users. In addition to using, they also help make the community better in their own way. Huge thanks to every user, contributor and committer.\\n\\n![image](images/users.jpg)\\n\\n### Mentor & PMC Words\\n\\n**@Liang Chen (Champion, Apache Member, Apache Incubator Mentor):** Congratulations to Kvrocks for becoming an ASF Top Project. Kvrocks community has become an outstanding NoSQL DB open source project in the Big Data ecosystem after more than 1 year of hard work and incubation in accordance with The Apache Way.\\n\\n**@Xiaoqiao He (Mentor, Apache Member, Apache Incubator Mentor):** Congratulations to Kvrocks on its successful graduation from the Apache Incubator. It was a pleasure to participate and witness Kvrocks successfully apply The Apache Way to community operations and project evolution after more than a year of incubation run and graduation. Graduation means a new beginning, I wish Kvrocks continues to build and prosper the data ecosystem, and look forward to Kvrocks creating more value.\\n\\n**@tison (Mentor, Apache Member, Apache Incubator Mentor):** Congratulations to Kvrocks, my first mentor-incubated project, for graduating from the incubator. I\'m excited to help the Kvrocks community grow under the guidance of The Apache Way, and to see how quickly the diversity of Kvrocks users and developers has grown over the past year. Graduating from the incubator is a small step for Kvrocks, but also a big step for all community members. Thank you and congratulations to everyone who has contributed to the development of Kvrocks!\\n\\n**@Yaroslav Stepanchuk (Kvrocks PMC Member): **It has been approximately a year since I first joined the Kvrocks community. From the very beginning, I was welcomed into a supportive environment, which made it incredibly easy for me, a complete newcomer to open-source, to make my initial contributions. Witnessing the project\'s journey and observing its current state fills me with pride. The fact that Kvrocks has successfully graduated from the Apache Incubator is a remarkable accomplishment that can be credited to all the contributors and users involved in the project. Reflecting on this milestone, I am truly amazed by the power of open-source collaboration. I firmly believe that the project\'s graduation will provide an additional boost, serving as a source of inspiration and leading to the expansion of our already exceptional community.\\n\\n**@Hulk Lin (Kvrocks PMC Member):** As one of the community maintainers, I am very happy that Kvrocks has become a top project in Apache after more than a year of incubation, and I am honored to work with many excellent contributors from home and abroad to maintain the project and the community. Graduation is just a new starting point, and I\'m looking forward to having more great people join us.\\n\\n**@Yuan Wang (Kvrocks PMC Member):** We\'re excited to graduate from the incubator and are happy to be working on Kvrocks with great contributors in the community, and we\'ll continue to pay attention to the feedback from the community to polish and optimize the product to make Kvrocks more stable and easy to use.\\n\\n**@PragmaTwice (Kvrocks PMC Member):** The ASF community\'s volunteer model and the community over code credo have helped me understand more about open source and make me feel very comfortable in the community. Coincidentally, Kvrocks graduated almost at the same time as me, so congratulations to me and the community!\\n\\n**@DongHui Liu (Kvrocks PMC Member):** We are happy to witness the whole journey of Kvrocks from the initial open source to the incubation in Apache to the successful graduation, and we are honored to maintain the Kvrocks project with the excellent contributors in the community. Graduation is an important milestone and the beginning of a new journey, we will continue to focus on our product capabilities and make Kvrocks better for the community. Good luck with Kvrocks!\\n\\n### Thanks\\n\\n- Thanks to the great efforts of every contributor, to make it possible for Kvrocks becoming a top-level project;\\n- Thank you to every Release Manager and to those who have helped Kvrocks release process. It is a great opportunity for community members to learn and practice the Apache Way from the release process;\\n- Thanks to the incubator mentors for their guidance and help in helping community members understand Apache\'s philosophy and code of conduct. In addition to being heavily involved in code development and community building;\\n- Finally, thank you to all users, your feedback and suggestions have had a significant impact on the development of Kvrocks, and this is what keeps the community going!\\n\\n### The End\\n\\nApache Kvrocks has significant potential for further development in terms of community influence and feature iteration. We work on maintaining an open and friendly atmosphere based on The Apache Way and attracting more excellent contributors. In the future, we plan to support more data types and optimize the experience of using Kvrocks in container environments. Additionally, we plan to simplify the operation and management of clusters."},{"id":"go-redis-kvrocks-opentelemetry","metadata":{"permalink":"/blog/go-redis-kvrocks-opentelemetry","editUrl":"https://github.com/apache/kvrocks-website/tree/main/blog/2022-11-20-go-redis-kvrocks-opentelemetry/index.md","source":"@site/blog/2022-11-20-go-redis-kvrocks-opentelemetry/index.md","title":"Getting started with Kvrocks and go-redis","description":"Learn how to use go-redis client to get started with Apache Kvrocks, a distributed key-value NoSQL database.","date":"2022-11-20T00:00:00.000Z","formattedDate":"November 20, 2022","tags":[],"readingTime":3.99,"hasTruncateMarker":true,"authors":[{"name":"Vladimir Mihailenco","title":"Grumpy Gopher","url":"https://github.com/vmihailenco","imageURL":"https://github.com/vmihailenco.png","key":"vmihailenco"}],"frontMatter":{"slug":"go-redis-kvrocks-opentelemetry","title":"Getting started with Kvrocks and go-redis","authors":["vmihailenco"]},"prevItem":{"title":"Kvrocks graduated as an Apache Top-Level-Project","permalink":"/blog/kvrocks-graduated-as-tlp"},"nextItem":{"title":"How we use RocksDB in Kvrocks?","permalink":"/blog/how-we-use-rocksdb-in-kvrocks"}},"content":"Learn how to use go-redis client to get started with Apache Kvrocks, a distributed key-value NoSQL database.\\n\\n\x3c!--truncate--\x3e\\n\\n## What is Kvrocks?\\n\\n[Apache Kvrocks](https://kvrocks.apache.org/) is a distributed key-value NoSQL database that uses RocksDB as a storage engine and is compatible with Redis protocol.\\n\\nYou can use Kvrocks as a drop-in replacement for Redis to store data on SSD, reducing storage costs and increasing capacity. For example, imagine taking one of the many existing Redis-based job queues and using them with Kvrocks and SSD storage.\\n\\nKvrocks supports most [Redis commands](/docs/supported-commands) including the `watch` command starting from v2.4.0. [Kvrocks Cluster](/docs/cluster) and [replication](/docs/replication) are available as well.\\n\\n## Getting started with Kvrocks\\n\\nYou can launch Kvrocks using Docker:\\n\\n```shell\\ndocker run -it -p 6666:6666 apache/kvrocks\\n```\\n\\nAnd start using it right away:\\n\\n```text\\nredis-cli -p 6666\\n\\n127.0.0.1:6666> get foo\\n(nil)\\n127.0.0.1:6666> set foo bar\\nOK\\n127.0.0.1:6666> get foo\\n\\"bar\\"\\n```\\n\\nYou can also [build](https://github.com/apache/kvrocks#build-and-run-kvrocks) Kvrocks with GCC yourself.\\n\\n## Connecting to Kvrocks from Go\\n\\nSince Kvrocks uses Redis-compatible protocol, you can use your favorite Redis client to work with Kvrocks, for example, [Go Redis client](https://redis.uptrace.dev/guide/go-redis.html):\\n\\n```go\\npackage main\\n\\nimport (\\n\\t\\"context\\"\\n\\t\\"github.com/redis/go-redis/v9\\"\\n)\\n\\nfunc main() {\\n\\tctx := context.Background()\\n\\n\\trdb := redis.NewClient(&redis.Options{\\n\\t\\tAddr: \\"localhost:6666\\",\\n\\t})\\n\\n\\terr := rdb.Set(ctx, \\"key\\", \\"value\\", 0).Err()\\n\\tif err != nil {\\n\\t\\tpanic(err)\\n\\t}\\n\\n\\tval, err := rdb.Get(ctx, \\"key\\").Result()\\n\\tif err != nil {\\n\\t\\tpanic(err)\\n\\t}\\n\\tfmt.Println(\\"key\\", val)\\n}\\n```\\n\\nPipelines, pub/sub, and even Lua scripts are working as well:\\n\\n```go\\nvar incrBy = redis.NewScript(`\\nlocal key = KEYS[1]\\nlocal change = ARGV[1]\\n\\nlocal value = redis.call(\\"GET\\", key)\\nif not value then\\n  value = 0\\nend\\n\\nvalue = value + change\\nredis.call(\\"SET\\", key, value)\\n\\nreturn value\\n`)\\n```\\n\\nYou can then run the script like this:\\n\\n```go\\nkeys := []string{\\"my_counter\\"}\\nvalues := []interface{}{+1}\\nnum, err := incrBy.Run(ctx, rdb, keys, values...).Int()\\n```\\n\\n## Monitoring Kvrocks\\n\\nMonitoring Kvrocks performance is crucial to ensure optimal operation and identify any potential bottlenecks or issues.\\n\\nOpenTelemetry provides instrumentation libraries and integrations for monitoring Kvrocks using its unified observability framework.\\n\\n### What is OpenTelemetry?\\n\\n[OpenTelemetry](https://opentelemetry.io/) is an open-source observability framework designed to standardize and simplify the collection, analysis, and export of telemetry data\\n\\nOpenTelemetry allows developers to collect and export telemetry data in a vendor agnostic way. With OpenTelemetry, you can instrument your application once and then add or change vendors without changing the instrumentation, for example, here is a list [DataDog competitors](https://uptrace.dev/blog/datadog-competitors.html) that support OpenTelemetry.\\n\\n### What is Uptrace?\\n\\nUptrace is an [OpenTelemetry APM](https://uptrace.dev/get/opentelemetry-apm.html) that supports [distributed tracing](https://uptrace.dev/opentelemetry/distributed-tracing.html), metrics, and logs. You can use it to monitor applications and set up automatic alerts to receive notifications via email, Slack, Telegram, and more.\\n\\nUptrace stores telemetry data in ClickHouse database. ClickHouse is an open source column-oriented database management system that is designed to process large volumes of data in real-time and to provide fast analytics and reporting capabilities.\\n\\n### Monitoring Kvrocks client\\n\\nYou can use OpenTelemetry and Uptrace together to monitor Kvrocks performance using the go-redis instrumentation:\\n\\n```go\\nimport \\"github.com/redis/go-redis/extra/redisotel/v9\\"\\n\\nrdb := redis.NewClient(&redis.Options{\\n\\tAddr: \\":6666\\",\\n})\\n\\nif err := redisotel.InstrumentTracing(rdb, redisotel.WithDBSystem(\\"kvrocks\\")); err != nil {\\n\\tpanic(err)\\n}\\nif err := redisotel.InstrumentMetrics(rdb, redisotel.WithDBSystem(\\"kvrocks\\")); err != nil {\\n\\tpanic(err)\\n}\\n```\\n\\nOnce the data reaches Uptrace, it will generate the following dashboard for you:\\n\\n![Uptrace DB dashboard](db-dashboard.png)\\n\\nBecause OpenTelemetry provides a vendor-agnostic approach, it allows you to choose an [OpenTelemetry backend](https://uptrace.dev/blog/opentelemetry-backend.html) that best suits your requirements, for example, Jaeger or Zipkin.\\n\\n### Monitoring Kvrocks server\\n\\nYou can also configure [OpenTelemetry Redis receiver](https://uptrace.dev/get/monitor/opentelemetry-redis.html) to monitor Kvrocks:\\n\\n```yaml\\nreceivers:\\n  redis/kvrocks:\\n    endpoint: \\"kvrocks:6666\\"\\n    collection_interval: 10s\\n```\\n\\nThe receiver works by parsing the output of `INFO` command and produces a number of useful metrics:\\n\\n![Redis Metrics](redis-metrics.png)\\n\\nSee [GitHub example](https://github.com/uptrace/uptrace/tree/master/example/kvrocks) for details.\\n\\nYou can also export the collected metrics to Prometheus using [OpenTelemetry Prometheus bridge](https://uptrace.dev/opentelemetry/prometheus-metrics.html)\\n\\n### Custom metrics\\n\\nUsing OpenTelemetry Metrics API, you can even create custom Kvrocks metrics. For example, the following function parses `used_disk_percent` to create `kvrocks.used_disk_percent` metric:\\n\\n```go\\nvar re = regexp.MustCompile(`used_disk_percent:\\\\s(\\\\d+)%`)\\n\\nfunc monitorKvrocks(ctx context.Context, rdb *redis.Client) error {\\n\\tmp := global.MeterProvider()\\n\\tmeter := mp.Meter(\\"github.com/uptrace/uptrace/example/kvrocks\\")\\n\\n\\tusedDiskPct, err := meter.AsyncFloat64().Gauge(\\n\\t\\t\\"kvrocks.used_disk_percent\\",\\n\\t\\tinstrument.WithUnit(\\"%\\"),\\n\\t)\\n\\tif err != nil {\\n\\t\\treturn err\\n\\t}\\n\\n\\treturn meter.RegisterCallback(\\n\\t\\t[]instrument.Asynchronous{\\n\\t\\t\\tusedDiskPct,\\n\\t\\t},\\n\\t\\tfunc(ctx context.Context) {\\n\\t\\t\\tpct, err := getUsedDiskPercent(ctx, rdb)\\n\\t\\t\\tif err != nil {\\n\\t\\t\\t\\totel.Handle(err)\\n\\t\\t\\t}\\n\\t\\t\\tusedDiskPct.Observe(ctx, pct, semconv.DBSystemKey.String(\\"kvrocks\\"))\\n\\t\\t},\\n\\t)\\n}\\n\\nfunc getUsedDiskPercent(ctx context.Context, rdb *redis.Client) (float64, error) {\\n\\tinfo, err := rdb.Info(ctx, \\"keyspace\\").Result()\\n\\tif err != nil {\\n\\t\\treturn 0, err\\n\\t}\\n\\n\\tm := re.FindStringSubmatch(info)\\n\\tif m == nil {\\n\\t\\treturn 0, errors.New(\\"can\'t find used_disk_percent metric\\")\\n\\t}\\n\\n\\tn, err := strconv.ParseInt(m[1], 10, 64)\\n\\tif err != nil {\\n\\t\\treturn 0, err\\n\\t}\\n\\n\\treturn float64(n) / 100, nil\\n}\\n```\\n\\nThe metric looks like this in Uptrace:\\n\\n![Redis Metrics](used-disk-percent.png)\\n\\nSee [OpenTelemetry Go Metrics API](https://uptrace.dev/opentelemetry/go-metrics.html) for details.\\n\\n## Useful links\\n\\n- [Getting started with Kvrocks](/docs/getting-started)\\n- [Golang Redis](https://redis.uptrace.dev/guide/go-redis.html)\\n- [Get started with Uptrace](https://uptrace.dev/get/get-started.html)"},{"id":"how-we-use-rocksdb-in-kvrocks","metadata":{"permalink":"/blog/how-we-use-rocksdb-in-kvrocks","editUrl":"https://github.com/apache/kvrocks-website/tree/main/blog/2021-12-26-how-we-use-rocksdb/index.md","source":"@site/blog/2021-12-26-how-we-use-rocksdb/index.md","title":"How we use RocksDB in Kvrocks?","description":"Kvrocks is an open-source key-value database that is based on rocksdb and compatible with Redis protocol. Intention to decrease the cost of memory and increase the capability while compared to Redis. We would focus on how we use RocksDB features to improve the performance of the Redis on disk. Hopes this helps people who want to improve performance on RocksDB.","date":"2021-12-26T00:00:00.000Z","formattedDate":"December 26, 2021","tags":[],"readingTime":7.935,"hasTruncateMarker":true,"authors":[{"name":"Hulk Lin","title":"Apache Kvrocks Founders","url":"https://github.com/git-hulk","imageURL":"https://github.com/git-hulk.png","key":"hulk"}],"frontMatter":{"slug":"how-we-use-rocksdb-in-kvrocks","title":"How we use RocksDB in Kvrocks?","authors":["hulk"]},"prevItem":{"title":"Getting started with Kvrocks and go-redis","permalink":"/blog/go-redis-kvrocks-opentelemetry"},"nextItem":{"title":"How to implement bitmap on RocksDB?","permalink":"/blog/how-to-implement-bitmap-on-rocksdb"}},"content":"Kvrocks is an open-source key-value database that is based on rocksdb and compatible with Redis protocol. Intention to decrease the cost of memory and increase the capability while compared to Redis. We would focus on how we use RocksDB features to improve the performance of the Redis on disk. Hopes this helps people who want to improve performance on RocksDB.\\n\\n\x3c!--truncate--\x3e\\n\\n## Background\\n\\nLet\'s have a look at how Kvrocks uses the RocksDB before introducing performance optimization. From the implementation side, Kvrocks would encode the Redis data structure into the key-values and write them into the different RocksDB\'s column families. There\'s five column family type in Kvrocks:\\n\\n* Metadata Column Family: used to store the metadata(expired, size..) for complex data structures like Hash/Set/ZSet/List, also string key-value was stored in this column family\\n* Subkey Column Family: used to store key-values for complex data structures were mentioned before\\n* ZSetScore Column Family: only store the score of the sorted set\\n* PubSub Column Family: used to store and propagate pubsub messages between the master and replicas\\n* Propagated Column Family: used to propagate commands between the master and replicas\\n\\nAlso, Kvrocks uses the RocksDB WAL to implement the replication, for more detail can see:\\n* [Kvrocks: An Open-Source Distributed Disk Key-Value Storage With Redis Protocol](https://kvrocks.medium.com/distributed-disk-key-value-storage-kvrocks-7bc5101c8585)\\n* [How to implement the Redis data structures on RocksDB](/community/data-structure-on-rocksdb)\\n\\nWe can have a glance at the Kvrocks architecture from 10,000 feet view\uff1a\\n\\n![image](architecture.jpeg)\\n\\n## How to profile RocksDB\\n\\n### Memtable Optimization\\n\\nCurrently, Kvrocks was using the SkipList Memtable. Compared with the HashSkipList Memtable, it has better performance when searching across multiple prefixes and uses less memory. Kvrocks also enabled the whole_key_filtering the option which would create a bloom filter for the key in the memtable, it can reduce the number of comparisons, thereby reducing the CPU usage during point query.\\nRelated configuration:\\n\\n```cpp\\nmetadata_opts.memtable_whole_key_filtering = true\\nmetadata_opts.memtable_prefix_bloom_size_ratio = 0.1\\n```\\n\\n### Data Block Optimization\\n\\nPreviously, Kvrocks used binary search when searching the data block, which may cause CPU cache miss and increase the CPU usage. As the point query was the most used scenario in the key-value service, so Kvrocks switched to the hash index to reduce the binary search comparisons. **Official test data shows that this feature can reduce CPU utilization by 21.8% and increase throughput by 10%, but it will increase space usage by 4.6%.** Compared with disk resources, CPU resources are more expensive. Under the trade-off, Kvrocks chose to enable the Hash index to improve the efficiency of point queries.\\n\\nRelated configuration:\\n\\n```cpp\\nBlockBasedTableOptions::data_block_index_type = DataBlockIndexType::kDataBlockBinaryAndHash\\nBlockBasedTableOptions::data_block_hash_table_util_ratio = 0.75\\n```\\n\\n### Filter/Index Block\\n\\nThe old version of RocksDB used Bloom Filter of BlockBasedFilter type by default. The basic mechanism is to generate a Filter for every 2KB of Key-Value data, and finally form a Filter array. When searching, first check the Index Block, and for the Data Block that may have the Key, then use the corresponding Filter Block to determine whether the key exists or not.\\n\\nThe new version of RocksDB optimizes the original Filter mechanism by introducing Full Filter. Each SST has a Filter, which can check whether the Key exists or not in the SST and avoid reading the Index Block. However, in the scenario with a large key number in the SST, the Filter Block and Index Block will still be larger. For 256MB SST, the size of Index and Filter Block is about 0.5MB and 5MB, which is much larger than Data Block (usually 4\u201332KB). In the most ideal case, when the Index/Filter Block is completely stored in memory, it will only be read once per SST life cycle, but when it competes with the Data Block for the Block Cache, it is likely to be re-read from the disk due to being evicted. Do it many times, resulting in very serious read amplification.\\n\\nKvrocks\' previous approach was to dynamically adjust the SST-related configuration so that the SST file will not be too large, thereby avoiding the Index/Filter Block from being too large. However, the problem with this mechanism is that when the amount of data is very large, too many SST files will take up more system resources and cause performance degradation. The new version of Kvrocks optimizes this and opens the related configuration of the Partitioned Block. The principle of the Partitioned Block is to add a secondary index to the Index/Filter Block. When reading the Index or Filter, the secondary index is first to read into the memory, and then Find the required partition Index Block according to the secondary index, and load it into the Block Cache.\\n\\nThe advantages of Partitioned Block are as follows:\\n\\n* Increase the cache hit rate: Large Index/Filter Block will pollute the cache space. The large Block will be partitioned, allowing the Index/Filter Block to be loaded at a finer granularity, thereby effectively using the cache space\\n* Improve cache efficiency: The partition Index/Filter Block will become smaller, the lock competition in the Cache will be further reduced, and the efficiency under high concurrency will be improved\\n* Reduce IO utilization: When the cache Miss of the index/filter partition, only a small partition needs to be loaded from the disk. Compared with the Index/Filter Block that reads the entire SST file, this will make the load on the disk smaller\\n\\nRelated configuration:\\n\\n```cpp\\nformat_version = 5\\nindex_type = IndexType::kTwoLevelIndexSearch\\nBlockBasedTableOptions::partition_filters = true\\ncache_index_and_filter_blocks = true\\npin_top_level_index_and_filter = true\\ncache_index_and_filter_blocks_with_high_priority = true\\npin_l0_filter_and_index_blocks_in_cache = true\\noptimize_filters_for_memory = true\\n```\\n\\n### Data compression optimization\\n\\nRocksDB compresses the data when it\'s placed on the disk. We compared and tested different compression algorithms on Kvrocks and found that different compression algorithms have a great impact on performance, especially when CPU resources are tight, which will significantly increase latency.\\n\\nThe following figure shows the test data of compression speed and compression ratio of different compression algorithms:\\n\\n![image](compression.jpeg)\\n\\nIn Kvrocks, compression is not set for the SST of the L0 and L1 layers, because these two layers have a small amount of data. Compressing the data at these levels cannot reduce a lot of disk space, but not compressing the data at these levels can save CPU. Each Compaction from L0 to L1 needs to access all files in L1. In addition, the range scan cannot use Bloom Filter, and it needs to find all files in L0. If you do not need to decompress when reading data in L0 and L1, and writing data in L0 and L1 do not need to be compressed, then these two frequent CPU-intensive operations will take up less CPU, compared to the disk space saved by compression, it is more profitable.\\n\\n**Considering the trade-off between compression speed and compression ratio, Kvrocks mainly chooses two algorithms, LZ4 and ZSTD.** For other levels, LZ4 is used because the compression algorithm is faster and the compression ratio is higher. RocksDB officially recommends using LZ4. For scenes with large data volume and low QPS, the last layer will be set to ZSTD to further reduce storage space and reduce costs. The advantage of ZSTD is that the compression ratio is higher and the compression speed is faster.\\n\\n### Cache optimization\\n\\nFor the simple data type (String type), the data is directly stored in Metadata CF, while for complex data types, only the metadata is stored in Metadata CF, and the actual data is stored in Subkey CF. Kvrocks previously allocated the same size of Block Cache to these two CFs by default. However, the online scene is complicated and the user\'s data type cannot be predicted, so it is not possible to allocate an appropriate Block Cache size to each CF in advance. If users use simple types and use complex types in different proportions, the Block Cache hit rate will decrease. Kvrocks shared the same large Block Cache to achieve a 30% improvement in the command rate of the Cache.\\nIn addition, Row Cache is also introduced to deal with the problem of hotkeys. RocksDB checks Row Cache first, then Block Cache. For scenes with hot spots, data will be stored in Row Cache first to further improve Cache utilization.\\n\\n### Key-Value Separation\\n\\nThe LSM storage engine will store the Key and Value together. During the compaction process, both Key and Value will be rewritten. When the Value is large, it will cause serious write amplification problems. In response to this problem, the  [WiscKey](https://www.usenix.org/system/files/conference/fast16/fast16-papers-lu.pdf) paper proposed a Key-Value separation scheme. Based on this paper, the industry also realized the KV separation of LSM-type storage engines, such as RocksDB\'s BlobDB, PingCAP\'s Titan engine, Quantum engine used by Baidu\'s UNDB.\\n\\nRocksDB 6.18 version re-implemented BlobDB (RocksDB\'s Key-Value separation scheme), integrated it into the main logic of RocksDB, and has been improving and optimizing BlobDB related features. **Kvrocks introduced this feature in 2.0.5 to deal with scenarios with large values. Tests show that when Kvrocks turns on the KV separation switch, for the scenario where Value is 10KB, the write performance is increased by 2.5 times, and the read performance is not attenuated; the larger the value, the greater the write performance improvement, and the write performance is improved when the Value is 50KB. 5 times.**\\n\\nRelated configuration:\\n\\n```cpp\\nColumnFamilyOptions.enable_blob_files = config_->RocksDB.enable_blob_files;\\nmin_blob_size = 4096\\nblob_file_size = 128M\\nblob_compression_type = lz4\\nenable_blob_garbage_collection = true\\nblob_garbage_collection_age_cutoff = 0.25\\nblob_garbage_collection_force_threshold = 0.8\\n```\\n\\n## Kvrocks Roadmap\\n\\n2021 is coming to an end, the related work of [Kvrocks 2.0](https://github.com/apache/kvrocks/projects/1) has been basically completed, and the plan of [Kvrocks 3.0](https://github.com/apache/kvrocks/projects/2) has also been listed on GitHub. This article lists the following two important features."},{"id":"how-to-implement-bitmap-on-rocksdb","metadata":{"permalink":"/blog/how-to-implement-bitmap-on-rocksdb","editUrl":"https://github.com/apache/kvrocks-website/tree/main/blog/2021-11-07-how-to-implement-bitmap-on-rocksdb/index.md","source":"@site/blog/2021-11-07-how-to-implement-bitmap-on-rocksdb/index.md","title":"How to implement bitmap on RocksDB?","description":"Most developers should be familiar with bitmap, in addition to the storage implementation for the bloom filter, and many databases also provide bitmap type indexes. For memory storage, the bitmap can be regarded as the special type of sparse bit array, which would not cause the read-write amplification issue (means read/write bytes far more than the request). While Redis supports bit-related operations on string types, it is a big challenge for disk KV-based storage like Kvrocks. So this article mainly discusses \\"How to reduce disk read/write amplification on RocksDB\\".","date":"2021-11-07T00:00:00.000Z","formattedDate":"November 7, 2021","tags":[],"readingTime":6.59,"hasTruncateMarker":true,"authors":[{"name":"Hulk Lin","title":"Apache Kvrocks Founders","url":"https://github.com/git-hulk","imageURL":"https://github.com/git-hulk.png","key":"hulk"}],"frontMatter":{"slug":"how-to-implement-bitmap-on-rocksdb","title":"How to implement bitmap on RocksDB?","authors":["hulk"]},"prevItem":{"title":"How we use RocksDB in Kvrocks?","permalink":"/blog/how-we-use-rocksdb-in-kvrocks"}},"content":"Most developers should be familiar with bitmap, in addition to the storage implementation for the bloom filter, and many databases also provide bitmap type indexes. For memory storage, the bitmap can be regarded as the special type of sparse bit array, which would not cause the read-write amplification issue (means read/write bytes far more than the request). While Redis supports bit-related operations on string types, it is a big challenge for disk KV-based storage like [Kvrocks](https://github.com/apache/kvrocks). So this article mainly discusses \\"**How to reduce disk read/write amplification on RocksDB**\\".\\n\\n\x3c!--truncate--\x3e\\n\\n## Why amplification occurs\\n\\nAmplification mainly comes from two aspects:\\n\\n* The hardware-level requires the smallest reading and writing unit\\n* How we organize the data distribution on the software-level\\n\\nTake SSD as an example, the smallest unit of reading/writing operation was the page (commonly 4KiB/8KiB/16KiB), and it would read or write one page even the request size was 1byte. Moreover, the way of SSD modification was Read-Modify-Write instead of in-place, which means SSD would read the page content and modify then write it out to another page, the old page would be reclaimed by GC. Similar to the following:\\n\\n![value-update-on-page](value-update-on-page.jpeg)\\n\\nAs we can see, a large random io was very unfriendly to the SSD disk, except for the performance issue, frequent erasing and writing will also seriously lead to the life of the SSD (random reads and writes are also unfriendly to HDDs, requiring constant seek and addressing). LSM-Tree alleviates such problems by changing random writes into sequential batch writes.\\n\\nThe read-write amplification at the software level mainly comes from the data organization method, and the degree of read-write amplification brought about by different organization methods will also vary greatly. Take RocksDB as an example here, RocksDB is Facebook based on Google LevelDB which enriches the multi-threading, Backup and Compaction, and many other very useful functions. To solve the problem of the disk write amplification, it also brings some space enlargement problems. Let\'s take a brief look at how LSM-Tree organizes data:\\n\\n![major-compaction](major-compaction.jpeg)\\n\\nLSM-Tree would create a new entry per write. For example in the above picture, the variable X is written 4 times successively, which are 0, 1, 2, 3. From the single variable X side, it was caused 4 times space amplification, those old spaces would be reclaimed on the background compaction. Similarly, deletion is achieved by inserting a record whose value is empty. The space size of each layer of LSM-Tree increases layer by layer. When the capacity reaches the limit, it will trigger compaction to merge to the next layer, and so on. Assuming that the maximum storage size of Level 0 is M Bytes, it increases layer by layer in 10 times and the maximum is 7 layers. Theoretically, the space enlargement is about 1.111111 times. Calculated as follows:\\n\\n```text\\namplification ratio = (1 + 10 + 100 +1000 + 10000 + 100000 + 1000000) * M / (1000000 * M)\\n```\\n\\nHowever, the magnification space rate is much larger than this theoretical value since the last layer generally cannot reach the maximum value. It is also mentioned in the RocksDB documentation. For details see also RockDB\'s blog [\\"Dynamic Level Size for Level-Based Compaction\\"](https://rocksdb.org/blog/2015/07/23/dynamic-level.html).\\n\\nIn addition, since RocksDB reads and writes are all based on key-value, the larger the value, the greater the read-write amplification may be. For example, suppose there is a JSON with a Value of 10 MiB. If you want to modify a field in this key, you need to read the entire JSON, modify and write it back again, which will cause huge read-write amplification. The paper [\\"WiscKey: Separating Keys from Values in SSD-conscious Storage\\"](https://www.usenix.org/system/files/conference/fast16/fast16-papers-lu.pdf) that optimizes the large Key-Value of LSM-Tree by separating key-value to reduce the write amplification problem caused by Compaction. The [titan](https://github.com/tikv/titan) project of TiKV is based on WiscKey paper to optimize RocksDB\'s write amplification in large key-value scenarios. RocksDB also implements this function in the community version, but it is still in an experimental stage.\\n\\n## Implement bitmap on RocksDB\\n\\nKvrocks is disk storage compatible with the Redis protocol implemented on RocksDB. It needs to support the bitmap data structure, so needs to implement the bitmap on RocksDB. In most scenes, the bitmap is used as sparse arrays, which means the offset written should be random, for the first time maybe 1, and the next offset maybe 1000000000 or more. Therefore, the implementation will face the above-mentioned amplification issue.\\n\\nA simple way is to regard the entire bitmap as a value, and read the value into the memory and then write it back when writing. Although this implementation is very simple, it would cause seriously amplification when the value was huge. In addition to the problem of effective space utilization, it may directly cause the entire service to be unavailable since we need to read and write back the entire value. Bitmap in Pika is such an implementation, but the maximum value is limit to 128 KiB. Limit the value size can avoid the above-mentioned extreme cases, but it will greatly affect the user scenes of bitmap.\\n\\nSince we know that the core problem is caused by a single key-value that is too large, the most direct way is to split the bitmap into multiple key-values, and control the single key-value size within a reasonable range, so the amplification is relatively under control. In the current implementation of Kvrocks, each key-value is divided into 1 KiB(8192 bits). The algorithm diagram is as follows:\\n\\n![bitmap-of-kvrocks](bitmap-of-kvrocks.jpeg)\\n\\nTake `setbit foo 8192002 1` as an example, the implementation steps are:\\n\\n* Calculate the key corresponding to the offset of `8192002`, because Kvrocks uses a value of 1 KiB, so the number of the key is `8192002/(1024*8)=1000`, so you can know the bit should be stored in the sub key `foo1000`.\\n* Then get the value corresponding to this key from RocksDB and calculate the offset in the segment, `8192002%8192` is equal to `2`, and then set the bit with the offset of 2 to 1.\\n* Finally, write the entire value back to RocksDB.\\n\\nA key point of this implementation is only read-write the limit part of the bitmap we need. Assuming that we have only executed `setbit` twice, `setbit foo 1 1` and `setbit foo 8192002 1`, then there will only read and write two keys `foo:0` and `foo:1000` in RocksDB, and the actual read-value size is only 2 KiB in total. It can be perfectly adapted to the sparse array scene like the bitmap, and will also not cause the problem of space enlargement due to sparse writing.\\n\\n> This idea is also similar to Linux\'s virtual memory/physical memory mapping strategy. For example, we request to malloc for 1GiB, and the operating system only allocates a piece of virtual memory address space. The physical memory was allocated when it is actually written will it trigger a page fault interrupt. That is, if the memory page has not been written, read-only will not cause physical memory allocation.\\n\\nGetBit is similar. It first calculates the key where the offset is located, and then reads the key from RocksDB.\\n\\n* If not exist, means that segment has not been written, returns 0 directly.\\n* If exists, read the Value and return the value of the corresponding bit.\\n\\nIn addition, the actual key-value size is also determined by the largest offset currently written. It would NOT always create a 1024 KiB key-value when there is a write. This can also help to optimize the read-write amplification problem within a single key-value in some degree. You can read [the source code of bitmap](https://github.com/apache/kvrocks/blob/unstable/src/types/redis_bitmap.cc) for more details.\\n\\n## Summary\\n\\nIt can be seen that to achieve the same thing in memory and disk was entirely different, the challenges are completely different. For disk-type services, it MUST continuously optimize the random read and write and space amplification issues. Familiar with the software was not enough, also requires to understand the hardware internal."}]}')}}]);